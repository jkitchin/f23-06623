[{"question": "For a linear regression model represented as y = b0 + b1.x1 + b2x2; with 10 data points (x, y), what will be the shape of the matrix X in X.b = y.", "type": "multiple_choice", "answers": [{"answer": "(10,3)", "correct": true}, {"answer": "(3, 1)", "correct": false}, {"answer": "(10, 1)", "correct": false}, {"answer": "(10, 10)", "correct": false}], "tag": "regression matrices", "lecture_file": "18_linear_regression"}, {"question": "Determine true or false:<br>Linear regression using numpy.linalg.lstsq() minimizes the errors, which also means it minimizes the residuals.", "type": "multiple_choice", "answers": [{"answer": "False ", "correct": false}, {"answer": "True ", "correct": true}, {"answer": "-", "correct": false}, {"answer": "-", "correct": false}], "tag": "regression numpy error residual", "lecture_file": "18_linear_regression"}, {"question": "Determine true or false:<br>When using numpy.linalg.lstsq(X, y), the X matrix must necessarily be a square matrix.", "type": "multiple_choice", "answers": [{"answer": "True ", "correct": false}, {"answer": "False ", "correct": true}, {"answer": "-", "correct": false}, {"answer": "-", "correct": false}], "tag": "regression numpy matrices", "lecture_file": "18_linear_regression"}, {"question": "For 2 linear regression models for the same data set, R-squared value of model-1 is 95% and that of model-2 is 65%. Which of the following statements are true?", "type": "multiple_choice", "answers": [{"answer": "Model 1 is a better represenation of the data set", "correct": false}, {"answer": "Model 2 is a better representation of the data set", "correct": false}, {"answer": "Both are equally good.", "correct": false}, {"answer": "Cannot say which model is better based on the R-squared values.", "correct": true}], "tag": "R_squared regression", "lecture_file": "18_linear_regression"}, {"question": "What will be the shape of the covariance matrix for a system which has 100 data points and is represented with the model: y = b0 + b1x1 + b2x2 + b3x3", "type": "multiple_choice", "answers": [{"answer": "(4, 4)", "correct": true}, {"answer": "(100, 100)", "correct": false}, {"answer": "(4, 100)", "correct": false}, {"answer": "(100, 4)", "correct": false}], "tag": "regression covariance", "lecture_file": "18_linear_regression"}, {"question": "To eliminate the least contributing independent variables in a linear regression model using regularization, a good choice can be to use", "type": "multiple_choice", "answers": [{"answer": "L2 regularization", "correct": false}, {"answer": "L1 regularization", "correct": true}, {"answer": "Both can work equally efficiently", "correct": false}, {"answer": "None of the above", "correct": false}], "tag": "regularization ridge lasso", "lecture_file": "18_linear_regression"}, {"question": "When using L1 regularization, if the value of lambda is extremely large as compared to the sum squared errors, then", "type": "multiple_choice", "answers": [{"answer": "We will get the most accurate model", "correct": false}, {"answer": "All the parameters might get reduced to zero", "correct": true}, {"answer": "It will have no effect on the regularization", "correct": false}, {"answer": "All the parameters will be reduced but never to zero", "correct": false}], "tag": "regularization lasso", "lecture_file": "18_linear_regression"}, {"question": "Standardizing the input variables", "type": "multiple_choice", "answers": [{"answer": "Results in average value of each variable equal to zero", "correct": false}, {"answer": "Eliminates the intercept", "correct": false}, {"answer": "Eliminates the effects of difference in absolute values of the variables", "correct": false}, {"answer": "All of the above", "correct": true}], "tag": "standardization", "lecture_file": "18_linear_regression"}, {"question": "Ridge regularization can be useful in", "type": "multiple_choice", "answers": [{"answer": "Eliminating the non-contributing features", "correct": false}, {"answer": "Reducing the errors arising from least-contributing features", "correct": true}, {"answer": "Reducing the penalty on the non-contributing coefficients as compared with Lasso", "correct": false}, {"answer": "All of the above", "correct": false}], "tag": "regularization ridge", "lecture_file": "18_linear_regression"}, {"question": "When using Ridge regularization, if the value of lambda = 0, then", "type": "multiple_choice", "answers": [{"answer": "All the coefficients will go to zero", "correct": false}, {"answer": "All the coefficients will go to infinity", "correct": false}, {"answer": "The final model is the same as a non-regularized linear regression model", "correct": true}, {"answer": "None of the above", "correct": false}], "tag": "regularization ridge", "lecture_file": "18_linear_regression"}]